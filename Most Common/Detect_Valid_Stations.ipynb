{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd953cd9",
   "metadata": {},
   "source": [
    "Import The Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82807372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b102e85",
   "metadata": {},
   "source": [
    "Directories Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75953e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = r\"D:\\Amin\\Education\\Course\\Data Analytics and Visualization (Winter 2021-2022)\\Project\\DataSet\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8252780",
   "metadata": {},
   "source": [
    "This part Will check if two or more stations have a same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d880d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateStartLocation(StartLocation, dataset, StartLat_string, StartLong_string, StartID_string):\n",
    "    for i in range(len(dataset)):\n",
    "        if((dataset[StartLat_string][i], dataset[StartLong_string][i]) not in StartLocation):\n",
    "            StartLocation[(dataset[StartLat_string][i], dataset[StartLong_string][i])] = dataset[StartID_string][i]\n",
    "        elif(StartLocation[(dataset[StartLat_string][i], dataset[StartLong_string][i])] != dataset[StartID_string][i]):\n",
    "            print(StartLocation[(dataset[StartLat_string][i], dataset[StartLong_string][i])], dataset[StartID_string][i])\n",
    "            print((dataset[StartLat_string][i], dataset[StartLong_string][i]), i)\n",
    "            print(\"Something Wrong Mate\")\n",
    "    return StartLocation\n",
    "    \n",
    "    \n",
    "def updateEndLocation(EndLocation, dataset, EndLat_string, EndLong_string, EndID_string):\n",
    "    temp = dataset[EndLat_string].isnull()\n",
    "    for i in range(len(dataset)):\n",
    "        if(temp[i] == True):\n",
    "            continue\n",
    "        if((dataset[EndLat_string][i], dataset[EndLong_string][i]) not in EndLocation): \n",
    "            EndLocation[(dataset[EndLat_string][i], dataset[EndLong_string][i])] = dataset[EndID_string][i]\n",
    "        elif(EndLocation[(dataset[EndLat_string][i], dataset[EndLong_string][i])] != dataset[EndID_string][i]):\n",
    "            print(EndLocation[(dataset[EndLat_string][i], dataset[EndLong_string][i])], dataset[EndID_string][i])\n",
    "            print((dataset[EndLat_string][i], dataset[EndLong_string][i]), i)\n",
    "            print(\"Something Wrong mate\")\n",
    "    return EndLocation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f658a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Start = {}\n",
    "End = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e459bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in os.listdir(DIR_PATH):\n",
    "    new_dir = DIR_PATH + \"\\\\\" + year\n",
    "    #if year == \"2013\" or year == \"2014\" or year == \"2015\" or year == \"2016\" or year == \"2017\" or year == \"2018\" or year == \"2019\":\n",
    "     #   continue\n",
    "    for month in os.listdir(new_dir):\n",
    "        FolderPath = new_dir + \"\\\\\" + month\n",
    "        if(month == \"ZipFile\"):\n",
    "            continue\n",
    "        for item in os.listdir(FolderPath):\n",
    "            DataSetPath = FolderPath + \"\\\\\" + item\n",
    "            data = pd.read_csv(DataSetPath)\n",
    "            if \"start station latitude\" in data:\n",
    "                updateStartLocation(Start, data, \"start station latitude\", \"start station longitude\", \"start station id\")\n",
    "                updateEndLocation(End, data, \"end station latitude\", \"end station longitude\", \"end station id\")\n",
    "            elif \"Start Station Latitude\" in data:\n",
    "                updateStartLocation(Start, data, \"Start Station Latitude\", \"Start Station Longitude\", \"Start Station ID\")\n",
    "                updateEndLocation(End, data, \"End Station Latitude\", \"End Station Longitude\", \"End Station ID\")\n",
    "            elif \"start_lat\" in data:\n",
    "                updateStartLocation(Start, data, \"start_lat\", \"start_lat\", \"start_station_id\")\n",
    "                updateEndLocation(End, data, \"end_lat\", \"end_lng\", \"end_station_id\")\n",
    "            else:\n",
    "                print(\"No Data this year\")\n",
    "            print(year, month)\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe628d",
   "metadata": {},
   "source": [
    "This part will make sure all of the data points are in a particular range, since we are only dealing with data from New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4439b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_latitude = 40.88226\n",
    "minimum_latitude =  40.604017\n",
    "\n",
    "maximum_longitude = -73.88145\n",
    "minimum_longitude = -74.03571\n",
    "\n",
    "valid_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8574b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dict(valid_dic, dataset, StartLat_string, StartLong_string, StartID_string, EndLat_string, EndLong_string, EndID_string):\n",
    "    for i in range(len(dataset)):\n",
    "        if((dataset[StartLat_string][i] >= minimum_latitude and dataset[StartLat_string][i] <=maximum_latitude) and (dataset[StartLong_string][i] >= minimum_longitude and dataset[StartLong_string][i] <=maximum_longitude)):\n",
    "            if(dataset[StartID_string][i] > -1):\n",
    "                valid_dic[(dataset[StartLat_string][i], dataset[StartLong_string][i])] = dataset[StartID_string][i]\n",
    "\n",
    "    temp = dataset[EndLat_string].isnull()\n",
    "    for i in range(len(dataset)):\n",
    "        if(temp[i] == True):\n",
    "            continue\n",
    "        if((dataset[StartLat_string][i] >= minimum_latitude and dataset[StartLat_string][i] <=maximum_latitude) and (dataset[StartLong_string][i] >= minimum_longitude and dataset[StartLong_string][i] <=maximum_longitude)):\n",
    "            if(dataset[EndID_string][i] > -1):\n",
    "                valid_dic[(dataset[StartLat_string][i], dataset[StartLong_string][i])] = dataset[EndID_string][i]\n",
    "    return valid_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe66744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 201306-citibike-tripdata\n",
      "2013 201307-citibike-tripdata\n",
      "2013 201308-citibike-tripdata\n",
      "2013 201309-citibike-tripdata\n",
      "2013 201310-citibike-tripdata\n",
      "2013 201311-citibike-tripdata\n",
      "2013 201312-citibike-tripdata\n",
      "2014 201401-citibike-tripdata\n",
      "2014 201402-citibike-tripdata\n",
      "2014 201403-citibike-tripdata\n",
      "2014 201404-citibike-tripdata\n",
      "2014 201405-citibike-tripdata\n",
      "2014 201406-citibike-tripdata\n",
      "2014 201407-citibike-tripdata\n",
      "2014 201408-citibike-tripdata\n",
      "2014 201409-citibike-tripdata\n",
      "2014 201410-citibike-tripdata\n",
      "2014 201411-citibike-tripdata\n",
      "2014 201412-citibike-tripdata\n",
      "2015 201501-citibike-tripdata\n",
      "2015 201502-citibike-tripdata\n",
      "2015 201503-citibike-tripdata\n",
      "2015 201504-citibike-tripdata\n",
      "2015 201505-citibike-tripdata\n",
      "2015 201506-citibike-tripdata\n",
      "2015 201507-citibike-tripdata\n",
      "2015 201508-citibike-tripdata\n",
      "2015 201509-citibike-tripdata\n",
      "2015 201510-citibike-tripdata\n",
      "2015 201511-citibike-tripdata\n",
      "2015 201512-citibike-tripdata\n",
      "2016 201601-citibike-tripdata\n",
      "2016 201602-citibike-tripdata\n",
      "2016 201603-citibike-tripdata\n",
      "2016 201604-citibike-tripdata\n",
      "2016 201605-citibike-tripdata\n",
      "2016 201606-citibike-tripdata\n",
      "2016 201607-citibike-tripdata\n",
      "2016 201608-citibike-tripdata\n",
      "2016 201609-citibike-tripdata\n",
      "2016 201610-citibike-tripdata\n",
      "2016 201611-citibike-tripdata\n",
      "2016 201612-citibike-tripdata\n",
      "2017 201701-citibike-tripdata.csv\n",
      "2017 201702-citibike-tripdata.csv\n",
      "2017 201703-citibike-tripdata.csv\n",
      "2017 201704-citibike-tripdata.csv\n",
      "2017 201705-citibike-tripdata.csv\n",
      "2017 201706-citibike-tripdata.csv\n",
      "2017 201707-citibike-tripdata.csv\n",
      "2017 201708-citibike-tripdata.csv\n",
      "2017 201709-citibike-tripdata.csv\n",
      "2017 201710-citibike-tripdata.csv\n",
      "2017 201711-citibike-tripdata.csv\n",
      "2017 201712-citibike-tripdata.csv\n",
      "2018 201801-citibike-tripdata.csv\n",
      "2018 201802-citibike-tripdata.csv\n",
      "2018 201803-citibike-tripdata.csv\n",
      "2018 201804-citibike-tripdata.csv\n",
      "2018 201805-citibike-tripdata.csv\n",
      "2018 201806-citibike-tripdata.csv\n",
      "2018 201807-citibike-tripdata.csv\n",
      "2018 201808-citibike-tripdata.csv\n",
      "2018 201809-citibike-tripdata.csv\n",
      "2018 201810-citibike-tripdata.csv\n",
      "2018 201811-citibike-tripdata.csv\n",
      "2018 201812-citibike-tripdata.csv\n",
      "2019 201901-citibike-tripdata.csv\n",
      "2019 201902-citibike-tripdata.csv\n",
      "2019 201903-citibike-tripdata.csv\n",
      "2019 201904-citibike-tripdata.csv\n",
      "2019 201905-citibike-tripdata.csv\n",
      "2019 201906-citibike-tripdata.csv\n",
      "2019 201907-citibike-tripdata.csv\n",
      "2019 201908-citibike-tripdata.csv\n",
      "2019 201909-citibike-tripdata.csv\n",
      "2019 201910-citibike-tripdata.csv\n",
      "2019 201911-citibike-tripdata.csv\n",
      "2019 201912-citibike-tripdata.csv\n",
      "2020 202001-citibike-tripdata.csv\n",
      "2020 202002-citibike-tripdata.csv\n",
      "2020 202003-citibike-tripdata.csv\n",
      "2020 202004-citibike-tripdata.csv\n",
      "2020 202005-citibike-tripdata.csv\n",
      "2020 202006-citibike-tripdata.csv\n",
      "2020 202007-citibike-tripdata.csv\n",
      "2020 202008-citibike-tripdata.csv\n",
      "2020 202009-citibike-tripdata.csv\n",
      "2020 202010-citibike-tripdata.csv\n",
      "2020 202011-citibike-tripdata.csv\n",
      "2020 202012-citibike-tripdata.csv\n",
      "2021 202101-citibike-tripdata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afadaei\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 202102-citibike-tripdata.csv\n",
      "2021 202103-citibike-tripdata.csv\n",
      "2021 202104-citibike-tripdata.csv\n",
      "2021 202105-citibike-tripdata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afadaei\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 202106-citibike-tripdata.csv\n",
      "2021 202107-citibike-tripdata.csv\n",
      "2021 202108-citibike-tripdata.csv\n",
      "2021 202109-citibike-tripdata.csv\n",
      "2021 202110-citibike-tripdata.csv\n",
      "2021 202111-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for year in os.listdir(DIR_PATH):\n",
    "    new_dir = DIR_PATH + \"\\\\\" + year\n",
    "    #if year == \"2013\" or year == \"2014\" or year == \"2015\" or year == \"2016\" or year == \"2017\" or year == \"2018\" or year == \"2019\":\n",
    "     #   continue\n",
    "    for month in os.listdir(new_dir):\n",
    "        FolderPath = new_dir + \"\\\\\" + month\n",
    "        if(month == \"ZipFile\"):\n",
    "            continue\n",
    "        for item in os.listdir(FolderPath):\n",
    "            DataSetPath = FolderPath + \"\\\\\" + item\n",
    "            data = pd.read_csv(DataSetPath)\n",
    "            if \"start station latitude\" in data:\n",
    "                valid_dic = add_dict(valid_dic, data, \"start station latitude\", \"start station longitude\", \"start station id\", \"end station latitude\", \"end station longitude\", \"end station id\")\n",
    "            elif \"Start Station Latitude\" in data:\n",
    "                valid_dic = add_dict(valid_dic, data, \"Start Station Latitude\", \"Start Station Longitude\", \"Start Station ID\", \"End Station Latitude\", \"End Station Longitude\", \"End Station ID\")\n",
    "            elif \"start_lat\" in data:\n",
    "                valid_dic = add_dict(valid_dic, data, \"start_lat\", \"start_lat\", \"start_station_id\", \"end_lat\", \"end_lng\", \"end_station_id\")\n",
    "            else:\n",
    "                print(\"No Data this year\")\n",
    "            print(year, month)\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a9848",
   "metadata": {},
   "source": [
    "Save The Validation Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a28457",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Valid_Stations4.npy', valid_dic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68b3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dictionary = np.load('Valid_Stations4.npy',allow_pickle='TRUE').item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
